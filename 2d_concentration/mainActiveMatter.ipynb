{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from the_well.data import WellDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from accelerate import notebook_launcher\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from torchvision.transforms import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  \n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  \n",
    "    num_epochs = 1000\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 10\n",
    "    mixed_precision = \"fp16\" \n",
    "    output_dir = \"ddpm-active-matter-128\"  \n",
    "\n",
    "    push_to_hub = False \n",
    "    overwrite_output_dir = True  \n",
    "    seed = 42\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dataset[1]['X'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WellDataset(\n",
    "    well_base_path=\"./../the_well/datasets/\",\n",
    "    well_dataset_name=\"active_matter\",\n",
    "    well_split_name=\"train\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_samples = [dataset[i] for i in torch.randint(len(dataset), (4,))]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, sample in enumerate(random_samples):\n",
    "    image = sample['input_fields'].squeeze(0).permute(2, 0, 1)[0, :, :].unsqueeze(0)\n",
    "    \n",
    "    image = avg_pool(image)\n",
    "    \n",
    "    im = axs[i].imshow(image.squeeze(), cmap='viridis')\n",
    "    axs[i].set_axis_off()\n",
    "    fig.colorbar(im, ax=axs[i])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size, \n",
    "    in_channels=1, \n",
    "    out_channels=1,  \n",
    "    layers_per_block=2,  \n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  \n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  \n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  \n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  \n",
    "        \"AttnUpBlock2D\",  \n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, \n",
    "                                clip_sample=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.resume_from_checkpoint:\n",
    "    checkpoint_dir = os.path.join(config.output_dir, f\"checkpoint-{epoch_start}\")\n",
    "    accelerator.load_state(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "epoch_start = 0\n",
    "\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    #loss_history = []  # List to store periodic loss values\n",
    "\n",
    "    for epoch in range(epoch_start, epoch_start + config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process, desc=f\"Epoch {epoch}\")\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0   \n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            images = avg_pool(batch['input_fields'].squeeze(1).permute(0, 3, 1, 2)[:, 0, :, :].unsqueeze(1))\n",
    "\n",
    "            noise = torch.randn(images.shape, device=images.device)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=images.device, dtype=torch.int64)\n",
    "            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Save loss value every 10 batches\n",
    "            if step % 10 == 0:\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "                \n",
    "                # Plot and save loss graph\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(loss_history)\n",
    "                plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "                plt.title('Training Loss')\n",
    "                plt.xlabel('Steps (x10)')\n",
    "                plt.ylabel('Loss (log scale)')\n",
    "                plt.savefig(os.path.join(config.output_dir, \"samples\", \"loss_graph.png\"))\n",
    "                plt.close()\n",
    "                save_dir = os.path.join(config.output_dir, \"samples\", \"checkpoint-{epoch}\")\n",
    "                accelerator.save_state(save_dir)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                pipeline.save_pretrained(config.output_dir)\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    pipeline.unet.eval()\n",
    "    \n",
    "    # Generate images\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(config.seed),\n",
    "        output_type=\"tensor\"\n",
    "    ).images\n",
    "    \n",
    "    # Create a matplotlib figure with 4x4 grid\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.cpu().squeeze().numpy()  \n",
    "        \n",
    "        axes[row, col].imshow(img) \n",
    "        axes[row, col].axis('off')\n",
    "        plt.colorbar(axes[row, col].images[0], ax=axes[row, col]) \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(f\"{test_dir}/{(epoch):04d}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)  \n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config.device\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "loss_history = notebook_launcher(train_loop, args, num_processes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "import custom_scheduler_active_matter\n",
    "importlib.reload(custom_scheduler_active_matter)\n",
    "import sys \n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import custom_scheduler_base\n",
    "importlib.reload(custom_scheduler_base)\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained('./ddpm-active-matter-128', use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# Load loss history from file \n",
    "with open(os.path.join(config.output_dir, 'loss_history.json'), 'r') as f:\n",
    "    loss_history = json.load(f)\n",
    "\n",
    "\n",
    "def sample_arbitrary(config, pipeline, num_inference_steps=1000):\n",
    "    images = torch.randn((16, 1, 128, 128), device=config.device)\n",
    "    \n",
    "    pipeline.scheduler.set_timesteps(num_inference_steps)\n",
    "    progress_bar = tqdm(total=num_inference_steps, desc=f\"yoyoyo\")\n",
    "\n",
    "    pipeline.unet.eval()\n",
    "    pipeline.unet.to(config.device)\n",
    "\n",
    "    for t in pipeline.scheduler.timesteps:\n",
    "        timestep = torch.full((1,), t, device=config.device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = pipeline.unet(images, timestep).sample\n",
    "        \n",
    "        images = pipeline.scheduler.step(noise_pred, t, images).prev_sample\n",
    "        progress_bar.update(1)\n",
    "    pipeline.unet.train()\n",
    "    return images.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.scheduler = custom_scheduler_active_matter.CustomScheduler(clip_sample=False,\n",
    "                                                                sample_max_value = 2.0,\n",
    "                                                                clip_sample_range = 2.0)\n",
    "sample_conditional = sample_arbitrary(config, pipe)\n",
    "\n",
    "pipe.scheduler = custom_scheduler_base.CustomSchedulerBase(clip_sample=False,\n",
    "                                                            sample_max_value = 2.0,\n",
    "                                                            clip_sample_range = 2.0)\n",
    "sample_unconditional = sample_arbitrary(config, pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 4 samples from each type\n",
    "samples_conditional = sample_conditional[:4].detach().clone()\n",
    "samples_unconditional = sample_unconditional[:4].detach().clone()\n",
    "random_samples = [dataset[i] for i in torch.randint(len(dataset), (4,))]\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(20, 12))\n",
    "\n",
    "for row, (samples, title) in enumerate([\n",
    "    (samples_conditional, \"Conditional Samples\"),\n",
    "    (samples_unconditional, \"Unconditional Samples\"),\n",
    "    (random_samples, \"Dataset Samples\")\n",
    "]):\n",
    "    # Add text to the left of the plots\n",
    "    fig.text(0.05, 0.85 - row*0.33, title, rotation=90, verticalalignment='center', fontsize=15)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if row < 2:  # For conditional and unconditional samples\n",
    "            image_np = sample.detach().cpu().numpy().squeeze()\n",
    "        else:  # For dataset samples\n",
    "            image = sample['input_fields'].squeeze(0).permute(2, 0, 1)[0, :, :].unsqueeze(0)\n",
    "            image_np = image.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        im = axs[row, i].imshow(image_np, cmap='viridis')\n",
    "        axs[row, i].set_axis_off()\n",
    "        fig.colorbar(im, ax=axs[row, i], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.07)  # Adjust left margin to make room for text\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10.0, 1.0, -5.0): 400 occurrences\n",
    "(10.0, 1.0, -4.0): 320 occurrences\n",
    "(10.0, 1.0, -3.0): 240 occurrences\n",
    "(10.0, 1.0, -2.0): 320 occurrences\n",
    "(10.0, 1.0, -1.0): 240 occurrences\n",
    "(10.0, 3.0, -5.0): 400 occurrences\n",
    "(10.0, 3.0, -4.0): 320 occurrences\n",
    "(10.0, 3.0, -3.0): 240 occurrences\n",
    "(10.0, 3.0, -2.0): 320 occurrences\n",
    "(10.0, 3.0, -1.0): 400 occurrences\n",
    "(10.0, 5.0, -5.0): 320 occurrences\n",
    "(10.0, 5.0, -4.0): 320 occurrences\n",
    "(10.0, 5.0, -3.0): 240 occurrences\n",
    "(10.0, 5.0, -2.0): 320 occurrences\n",
    "(10.0, 5.0, -1.0): 240 occurrences\n",
    "(10.0, 7.0, -5.0): 320 occurrences\n",
    "(10.0, 7.0, -4.0): 400 occurrences\n",
    "(10.0, 7.0, -3.0): 240 occurrences\n",
    "(10.0, 7.0, -2.0): 240 occurrences\n",
    "(10.0, 7.0, -1.0): 400 occurrences\n",
    "(10.0, 9.0, -5.0): 400 occurrences\n",
    "(10.0, 9.0, -4.0): 320 occurrences\n",
    "(10.0, 9.0, -3.0): 320 occurrences\n",
    "(10.0, 9.0, -2.0): 400 occurrences\n",
    "(10.0, 9.0, -1.0): 240 occurrences\n",
    "(10.0, 11.0, -5.0): 400 occurrences\n",
    "(10.0, 11.0, -4.0): 80 occurrences\n",
    "(10.0, 11.0, -3.0): 240 occurrences\n",
    "(10.0, 11.0, -2.0): 240 occurrences\n",
    "(10.0, 11.0, -1.0): 320 occurrences\n",
    "(10.0, 13.0, -5.0): 320 occurrences\n",
    "(10.0, 13.0, -4.0): 400 occurrences\n",
    "(10.0, 13.0, -3.0): 240 occurrences\n",
    "(10.0, 13.0, -2.0): 320 occurrences\n",
    "(10.0, 13.0, -1.0): 320 occurrences\n",
    "(10.0, 15.0, -5.0): 160 occurrences\n",
    "(10.0, 15.0, -4.0): 320 occurrences\n",
    "(10.0, 15.0, -3.0): 400 occurrences\n",
    "(10.0, 15.0, -2.0): 320 occurrences\n",
    "(10.0, 15.0, -1.0): 320 occurrences\n",
    "(10.0, 17.0, -5.0): 400 occurrences\n",
    "(10.0, 17.0, -4.0): 240 occurrences\n",
    "(10.0, 17.0, -3.0): 240 occurrences\n",
    "(10.0, 17.0, -2.0): 400 occurrences\n",
    "(10.0, 17.0, -1.0): 400 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = WellDataset(\n",
    "    well_base_path=\"./../the_well/datasets/\",\n",
    "    well_dataset_name=\"active_matter\",\n",
    "    well_split_name=\"test\"\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=config.train_batch_size, shuffle=True)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\"\"\"\n",
    "train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "#test_dataloader = DataLoader(dataset_test, batch_size=config.train_batch_size, shuffle=True)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "images = avg_pool(batch['input_fields'].squeeze(1).permute(0, 3, 1, 2)[:, 0, :, :].unsqueeze(1)) # (16, 1, 128, 128)\n",
    "target = batch['constant_scalars'][:, 1].to(torch.int64)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL FOR FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate samples from both models and save them\n",
    "import os\n",
    "\n",
    "# Create temp directory if it doesn't exist\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# Generate and save conditional samples\n",
    "pipe.scheduler = custom_scheduler_active_matter.CustomScheduler(clip_sample=False,\n",
    "                                                                sample_max_value = 2.0,\n",
    "                                                                clip_sample_range = 2.0)\n",
    "conditional_samples = sample_arbitrary(config, pipe)\n",
    "torch.save(conditional_samples, \"temp/conditional_samples.pt\")\n",
    "\n",
    "# Generate and save unconditional samples \n",
    "pipe.scheduler = custom_scheduler_base.CustomSchedulerBase(clip_sample=False,\n",
    "                                                            sample_max_value = 2.0,\n",
    "                                                            clip_sample_range = 2.0)\n",
    "unconditional_samples = sample_arbitrary(config, pipe)\n",
    "torch.save(unconditional_samples, \"temp/unconditional_samples.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from the_well.data import WellDataset\n",
    "from torch.utils.data import DataLoader\n",
    "dataset_test = WellDataset(\n",
    "    well_base_path=\"./../the_well/datasets/\",\n",
    "    well_dataset_name=\"active_matter\",\n",
    "    well_split_name=\"test\"\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=config.train_batch_size, shuffle=True)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "# Load your generated samples\n",
    "conditional_samples = torch.load(\"temp/conditional_samples.pt\")  # shape: (16, 1, 128, 128)\n",
    "unconditional_samples = torch.load(\"temp/unconditional_samples.pt\")\n",
    "\n",
    "# Get real samples from the test dataloader\n",
    "real_batch = next(iter(test_dataloader))\n",
    "real_samples = avg_pool(real_batch['input_fields'].squeeze(1).permute(0, 3, 1, 2)[:, 0, :, :].unsqueeze(1))  # shape: (16, 1, 128, 128)\n",
    "\n",
    "# Define a transform to upsample and convert 1-channel to 3-channel\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),                     # Required for Inception-v3\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))     # Convert 1-channel to 3-channel\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "def prepare_images(batch):\n",
    "    # batch shape: (B, 1, 128, 128)\n",
    "    processed = []\n",
    "    for img in batch:\n",
    "        # img: (1, 128, 128)\n",
    "        img_resized = transforms.functional.resize(img, (299, 299))  # Resize\n",
    "        img_rgb = img_resized.repeat(3, 1, 1)  # Convert to 3-channel\n",
    "        processed.append(img_rgb)\n",
    "    return torch.stack(processed)\n",
    "\n",
    "\n",
    "real_prepared = prepare_images(real_samples)\n",
    "cond_prepared = prepare_images(conditional_samples)\n",
    "uncond_prepared = prepare_images(unconditional_samples)\n",
    "\n",
    "# Initialize FID metric\n",
    "fid = FrechetInceptionDistance(normalize=True)\n",
    "\n",
    "# Compute FID for conditional\n",
    "fid.update(real_prepared, real=True)\n",
    "fid.update(cond_prepared, real=False)\n",
    "fid_conditional = fid.compute()\n",
    "fid.reset()\n",
    "\n",
    "# Compute FID for unconditional\n",
    "fid.update(real_prepared, real=True)\n",
    "fid.update(uncond_prepared, real=False)\n",
    "fid_unconditional = fid.compute()\n",
    "\n",
    "print(\"FID (Conditional):\", fid_conditional.item())\n",
    "print(\"FID (Unconditional):\", fid_unconditional.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_snr(real, generated):\n",
    "    \"\"\"\n",
    "    Computes Signal-to-Noise Ratio (SNR) between real and generated images.\n",
    "    Both tensors should have shape: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    # Ensure both inputs are float32\n",
    "    real = real.float()\n",
    "    generated = generated.float()\n",
    "    \n",
    "    # Compute signal power and noise power\n",
    "    signal_power = torch.mean(real ** 2, dim=[1, 2, 3])\n",
    "    noise_power = torch.mean((real - generated) ** 2, dim=[1, 2, 3])\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    snr = 10 * torch.log10(signal_power / (noise_power + 1e-8))\n",
    "    \n",
    "    # Return average SNR across batch\n",
    "    return snr.mean().item()\n",
    "\n",
    "# Assume real_samples and generated_samples are (B, 1, 128, 128)\n",
    "snr_conditional = compute_snr(real_samples, conditional_samples)\n",
    "snr_unconditional = compute_snr(real_samples, unconditional_samples)\n",
    "\n",
    "print(\"SNR (Conditional):\", snr_conditional)\n",
    "print(\"SNR (Unconditional):\", snr_unconditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Initialize SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure()  # data_range=1.0 assumes images in [0, 1]\n",
    "\n",
    "# Compute SSIM for conditional and unconditional samples\n",
    "ssim_conditional = ssim_metric(conditional_samples, real_samples).item()\n",
    "ssim_unconditional = ssim_metric(unconditional_samples, real_samples).item()\n",
    "\n",
    "print(\"SSIM (Conditional):\", ssim_conditional)\n",
    "print(\"SSIM (Unconditional):\", ssim_unconditional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Classification Model for the 2D concentration dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Input: (batch_size, 1, 128, 128)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = SimpleCNN(num_classes=5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "def remap_labels(original_labels):\n",
    "    # Convert 1,3,5,...,17 to 0,1,2,...,8\n",
    "    return (original_labels - 1) // 2\n",
    "def remap_labels2(original_labels):\n",
    "    return -(original_labels + 1) \n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Get the inputs and targets\n",
    "        inputs = batch['input_fields'].squeeze(1).permute(0, 3, 1, 2)[:, 0, :, :].unsqueeze(1)\n",
    "        inputs = torch.nn.functional.avg_pool2d(inputs, kernel_size=2, stride=2)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = remap_labels2(batch['constant_scalars'][:, 2].to(torch.int64))\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss, acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}, Accuracy: {acc:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), \"cnn_classifier.pth\")\n",
    "print(\"Model saved to cnn_classifier.pth\") \n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
